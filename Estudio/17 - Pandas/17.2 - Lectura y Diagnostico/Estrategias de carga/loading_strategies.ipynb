{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup: Generación de Archivos de Prueba\n",
    "\n",
    "Ejecuta esto una sola vez para crear los archivos .csv y .parquet con los que practicaremos."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T22:29:03.623365700Z",
     "start_time": "2026-02-10T22:29:02.783743200Z"
    }
   },
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "import os "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T22:29:04.680478400Z",
     "start_time": "2026-02-10T22:29:04.493250Z"
    }
   },
   "source": [
    "#1. Creamos un DataFrame grande(10,000 filas)\n",
    "df_dummy = pd.DataFrame({\n",
    "    'fecha': pd.date_range(start='2020-01-01', periods=10000,freq='h'),\n",
    "    'producto': np.random.choice(['Laptop','Mouse','Teclado','Monitor'], 10000),\n",
    "    'precio': np.random.randint(20,2000,10000),\n",
    "    'cliente_id':np.random.randint(1000, 9999, 10000),\n",
    "    'comentarios': ['Texto largo de relleno para ocupar memoria RAM innecesariamente'] * 10000\n",
    "})\n",
    "\n",
    "#2. Guardamos en formato CSV(Texto Lento)\n",
    "df_dummy.to_csv('Archivos/ventas_grandes.csv', index=False)\n",
    "\n",
    "#3. Guardamos en formato Parquet(Binario rápido)\n",
    "# Nota: Si te da error, necesitas instalar: pip install pyarrow\n",
    "try:\n",
    "    df_dummy.to_parquet('Archivos/ventas_grande.parquet')\n",
    "    print(\"Archivos 'ventas_grande.csv' y 'ventas_grande.parquet' creados.\")\n",
    "except ImportError:\n",
    "    print(\"No se pudo crear Parquet. Instala pyarrow. Solo usaremos CSV.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos 'ventas_grande.csv' y 'ventas_grande.parquet' creados.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo 1: Estrategia de Selección (usecols)\n",
    "\n",
    "Teoría: No cargues basura. Si solo necesitas analizar precios por producto, ¿para qué cargar los comentarios largos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CARGA INGENUA (Carga todo) ---\n",
      "Memoria Full:1282679 bytes\n",
      "\n",
      "--- CARGA ESTRATÉGICA (Solo columnas útiles) ---\n",
      "Memoria Optimizada:222679 bytes\n",
      "Ahorro de Memoria:82.64%\n"
     ]
    }
   ],
   "source": [
    "print(\"--- CARGA INGENUA (Carga todo) ---\")\n",
    "df_full=pd.read_csv('Archivos/ventas_grandes.csv')\n",
    "print(f\"Memoria Full:{df_full.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "print(\"\\n--- CARGA ESTRATÉGICA (Solo columnas útiles) ---\")\n",
    "#usecols: Solo traemos lo que necesitamos\n",
    "df_opt=pd.read_csv('Archivos/ventas_grandes.csv', usecols=['producto','precio'])\n",
    "print(f\"Memoria Optimizada:{df_opt.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "#Calculo de ahorro\n",
    "ahorro=1-(df_opt.memory_usage(deep=True).sum()/df_full.memory_usage(deep=True).sum())\n",
    "print(f\"Ahorro de Memoria:{ahorro:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación: Al ignorar la columna 'comentarios', la carga es instantánea y ocupa una fracción de la RAM. Esta es la técnica #1 de Matt Harrison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo 2: Optimización de Tipos (dtype y parse_dates)\n",
    "\n",
    "Teoría: Evitar que Pandas \"adivine\" los tipos. Le decimos explícitamente qué es cada cosa para comprimir memoria al vuelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CARGA INTELIGENTE (Tipos definidos) ---\n",
      "<class 'pandas.DataFrame'>\n",
      "DatetimeIndex: 10000 entries, 2020-01-01 00:00:00 to 2021-02-20 15:00:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   producto  10000 non-null  category\n",
      " 1   precio    10000 non-null  int16   \n",
      "dtypes: category(1), int16(1)\n",
      "memory usage: 107.6 KB\n",
      "None\n",
      "\n",
      "Observa: El índice ya es Datetime y 'producto' es Category.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- CARGA INTELIGENTE (Tipos definidos) ---\")\n",
    "\n",
    "#Definimos la estrategia\n",
    "tipos_optimizados = {\n",
    "    'producto': 'category', #De object a category (Ahorro Masivo)\n",
    "    'precio': 'int16'   #De int64 a int16 (Ahorro de Bits)\n",
    "}\n",
    "\n",
    "df_smart=pd.read_csv('Archivos/ventas_grandes.csv',\n",
    "                    usecols=['fecha','producto','precio'],  #Filtrado vertical\n",
    "                    dtype=tipos_optimizados,    #compresión de tipos\n",
    "                    parse_dates=['fecha'],  #Parseo temporal inmediato\n",
    "                    index_col='fecha'   #Indexación automática\n",
    ")\n",
    "\n",
    "print(df_smart.info())\n",
    "print(\"\\nObserva: El índice ya es Datetime y 'producto' es Category.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación: Hemos combinado 4 pasos en 1. El DataFrame nace limpio, indexado y optimizado. No hace falta hacer limpieza posterior (astype) ni set_index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CARGA POR LOTES (Chunking) ---\n",
      "Objeto recibido:<class 'pandas.io.parsers.readers.TextFileReader'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 1: Procesadas 2000filas. Suma Parcial:2003543\n",
      "Lote 2: Procesadas 2000filas. Suma Parcial:1979053\n",
      "Lote 3: Procesadas 2000filas. Suma Parcial:1991216\n",
      "Lote 4: Procesadas 2000filas. Suma Parcial:1991630\n",
      "Lote 5: Procesadas 2000filas. Suma Parcial:2020899\n",
      "\n",
      "Venta Total Calculada (Sin cargar todo a la vez): 9986341\n"
     ]
    }
   ],
   "source": [
    "print(\"--- CARGA POR LOTES (Chunking) ---\")\n",
    "\n",
    "#Supongamos que 10000 filas es \"demasiado\" para mi RAM\n",
    "#Cargaremos de 2000 en 2000\n",
    "\n",
    "chunck_iterator=pd.read_csv('Archivos/ventas_grandes.csv',chunksize=2000)\n",
    "\n",
    "print(f\"Objeto recibido:{type(chunck_iterator)}\") #No es un DataFrame, es un TextFileReader\n",
    "total_ventas=0\n",
    "\n",
    "#Iteramos como si fuera una lista\n",
    "for i,lote_df in enumerate(chunck_iterator):\n",
    "    # En cada vuelta, 'lote_df' es un pequeño DataFrame de 2000 filas\n",
    "    suma_lote=lote_df['precio'].sum()\n",
    "    total_ventas+=suma_lote\n",
    "    print(f\"Lote {i+1}: Procesadas {len(lote_df)}filas. Suma Parcial:{suma_lote}\")\n",
    "\n",
    "print(f\"\\nVenta Total Calculada (Sin cargar todo a la vez): {total_ventas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación: Esta es la única forma de procesar archivos de Gigabytes en una laptop normal. Nunca cargas todo el archivo, solo fragmentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Velocidad Binaria (read_parquet)\n",
    "\n",
    "Teoría: Leer texto (CSV) es lento porque la CPU tiene que leer letra por letra. Parquet es binario, la CPU solo copia memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DUELO DE VELOCIDAD: CSV vs PARQUET ---\n",
      "Tiempo CSV: 0.0384 segundos\n",
      "Tiempo Parquet: 0.0847 segundos\n",
      "Parquet es 0.5 veces más rápido.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- DUELO DE VELOCIDAD: CSV vs PARQUET ---\")\n",
    "\n",
    "#1. Medir CSV\n",
    "inicio=time.time()\n",
    "df_csv=pd.read_csv('Archivos/ventas_grandes.csv')\n",
    "fin=time.time()\n",
    "tiempo_csv=fin-inicio\n",
    "print(f\"Tiempo CSV: {tiempo_csv:.4f} segundos\")\n",
    "\n",
    "#2. Medir Parquet (Si tienes pyrrow instalado)\n",
    "try:\n",
    "    inicio=time.time()\n",
    "    df_parquet=pd.read_parquet(\"Archivos/ventas_grande.parquet\")\n",
    "    fin=time.time()\n",
    "    tiempo_parquet=fin-inicio\n",
    "    print(f\"Tiempo Parquet: {tiempo_parquet:.4f} segundos\")\n",
    "\n",
    "    print(f\"Parquet es {tiempo_csv / tiempo_parquet:.1f} veces más rápido.\")\n",
    "except:\n",
    "    print(\"Salto paso Parquet (falta librería).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicación: En archivos grandes reales, Parquet suele ser entre 10 y 50 veces más rápido que CSV y ocupa la mitad de espacio en disco. Es el formato profesional por excelencia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
